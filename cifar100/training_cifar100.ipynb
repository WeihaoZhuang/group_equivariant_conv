{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "#!/usr/bin/env\tpython3\n",
    "\n",
    "\"\"\" train network using pytorch\n",
    "\n",
    "author baiyu\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "#from dataset import *\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from conf import settings\n",
    "from utils import get_network, get_training_dataloader, get_test_dataloader, WarmUpLR\n",
    "\n",
    "def train(epoch):\n",
    "    train_loss = 0.0\n",
    "    net.train()\n",
    "    for batch_index, (images, labels) in enumerate(cifar100_training_loader):\n",
    "        if epoch <= args.warm:\n",
    "            warmup_scheduler.step()\n",
    "\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        labels = labels.cuda()\n",
    "        images = images.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        n_iter = (epoch - 1) * len(cifar100_training_loader) + batch_index + 1\n",
    "\n",
    "#         last_layer = list(net.children())[-1]\n",
    "#         for name, para in last_layer.named_parameters():\n",
    "#             if 'weight' in name:\n",
    "#                 writer.add_scalar('LastLayerGradients/grad_norm2_weights', para.grad.norm(), n_iter)\n",
    "#             if 'bias' in name:\n",
    "#                 writer.add_scalar('LastLayerGradients/grad_norm2_bias', para.grad.norm(), n_iter)\n",
    "\n",
    "    print('Training Epoch: {epoch} \\tLoss: {:0.4f}\\tLR: {:0.6f}'.format(\n",
    "        train_loss/len(cifar100_training_loader.dataset),\n",
    "        optimizer.param_groups[0]['lr'],\n",
    "        epoch=epoch,\n",
    "    ))\n",
    "#     update training loss for each iteration\n",
    "    writer.add_scalar('Train/loss', train_loss/len(cifar100_training_loader.dataset), epoch)\n",
    "\n",
    "    for name, param in net.named_parameters():\n",
    "        layer, attr = os.path.splitext(name)\n",
    "        attr = attr[1:]\n",
    "        writer.add_histogram(\"{}/{}\".format(layer, attr), param, epoch)\n",
    "        \n",
    "def eval_training(epoch):\n",
    "    net.eval()\n",
    "\n",
    "    test_loss = 0.0 # cost function error\n",
    "    correct = 0.0\n",
    "\n",
    "    for (images, labels) in cifar100_test_loader:\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        outputs = net(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, preds = outputs.max(1)\n",
    "        correct += preds.eq(labels).sum()\n",
    "\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {:.4f}'.format(\n",
    "        test_loss / len(cifar100_test_loader.dataset),\n",
    "        correct.float()*100 / len(cifar100_test_loader.dataset)\n",
    "    ))\n",
    "    print()\n",
    "\n",
    "    #add informations to tensorboard\n",
    "    writer.add_scalar('Test/Average loss', test_loss / len(cifar100_test_loader.dataset), epoch)\n",
    "    writer.add_scalar('Test/Accuracy', correct.float()*100 / len(cifar100_test_loader.dataset), epoch)\n",
    "\n",
    "    return correct.float()*100 / len(cifar100_test_loader.dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args():\n",
    "    net = 'resnet18'\n",
    "    gpu = True\n",
    "    w = 2\n",
    "    b = 128\n",
    "    s = True\n",
    "    warm = 1\n",
    "    lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weihao_zhuang/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 \tLoss: 0.0318\tLR: 0.100000\n",
      "Test set: Average loss: 0.0289, Accuracy: 0.1355\n",
      "\n",
      "Training Epoch: 2 \tLoss: 0.0271\tLR: 0.000800\n",
      "Test set: Average loss: 0.0264, Accuracy: 0.1870\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# net = get_network(args, use_gpu=args.gpu)\n",
    "net = torchvision.models.resnet18(num_classes = 100).cuda()\n",
    "\n",
    "#data preprocessing:\n",
    "cifar100_training_loader = get_training_dataloader(\n",
    "    settings.CIFAR100_TRAIN_MEAN,\n",
    "    settings.CIFAR100_TRAIN_STD,\n",
    "    num_workers=args.w,\n",
    "    batch_size=args.b,\n",
    "    shuffle=args.s\n",
    ")\n",
    "\n",
    "cifar100_test_loader = get_test_dataloader(\n",
    "    settings.CIFAR100_TRAIN_MEAN,\n",
    "    settings.CIFAR100_TRAIN_STD,\n",
    "    num_workers=args.w,\n",
    "    batch_size=args.b,\n",
    "    shuffle=args.s\n",
    ")\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
    "train_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=settings.MILESTONES, gamma=0.2) #learning rate decay\n",
    "iter_per_epoch = len(cifar100_training_loader)\n",
    "warmup_scheduler = WarmUpLR(optimizer, iter_per_epoch * args.warm)\n",
    "checkpoint_path = os.path.join(settings.CHECKPOINT_PATH, args.net, settings.TIME_NOW)\n",
    "\n",
    "#use tensorboard\n",
    "if not os.path.exists(settings.LOG_DIR):\n",
    "    os.mkdir(settings.LOG_DIR)\n",
    "    \n",
    "log_dir = 'test_resnet18'\n",
    "writer = SummaryWriter(log_dir=\"./log/\"+log_dir)\n",
    "# input_tensor = torch.Tensor(12, 3, 32, 32).cuda()\n",
    "# writer.add_graph(net, Variable(input_tensor, requires_grad=True))\n",
    "\n",
    "#create checkpoint folder to save model\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint_path = os.path.join(checkpoint_path, '{net}-{epoch}-{type}.pth')\n",
    "\n",
    "best_acc = 0.0\n",
    "for epoch in range(1, settings.EPOCH):\n",
    "    if epoch > args.warm:\n",
    "        train_scheduler.step(epoch)\n",
    "\n",
    "    train(epoch)\n",
    "    acc = eval_training(epoch)\n",
    "\n",
    "#     #start to save best performance model after learning rate decay to 0.01 \n",
    "#     if epoch > settings.MILESTONES[1] and best_acc < acc:\n",
    "#         torch.save(net.state_dict(), checkpoint_path.format(net=args.net, epoch=epoch, type='best'))\n",
    "#         best_acc = acc\n",
    "#         continue\n",
    "\n",
    "#     if not epoch % settings.SAVE_EPOCH:\n",
    "#         torch.save(net.state_dict(), checkpoint_path.format(net=args.net, epoch=epoch, type='regular'))\n",
    "\n",
    "# writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
